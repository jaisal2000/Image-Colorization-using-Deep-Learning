{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moneyjais/Image-Colorization-using-Deep-Learning/blob/main/Copy_of_Major.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlA8mg9R72HA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "import torch\n",
        "import torchsummary\n",
        "import pydot\n",
        "import graphviz\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import tensorflow as tf ;\n",
        "from tensorflow.keras.utils import plot_model ;\n",
        "from tensorflow.keras import layers, models, losses, optimizers ;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PE3wuiC79Hn",
        "outputId": "f9bb42d2-ad11-4fd1-ed3e-2f0fed75c324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIKOFugE8BkR"
      },
      "outputs": [],
      "source": [
        "dataset_path = os.path.join('/content/drive', 'MyDrive', 'coco_sample', 'train_sample')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JRkiwy-_Lfs"
      },
      "outputs": [],
      "source": [
        "images = glob.glob(os.path.join(dataset_path, \"*.jpg\"))\n",
        "images = sorted(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f5Ny24K_YOZ",
        "outputId": "a9562123-a63a-4189-cd1a-e4ffb4e99b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000 2000\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(123)\n",
        "paths_subset = np.random.choice(images, 10_000, replace=False) #choosing random 10000 images\n",
        "\n",
        "rand_idxs = np.random.permutation(10_000)\n",
        "train_idxs = rand_idxs[:8000]  #first 8000 images as training set\n",
        "val_idxs = rand_idxs[8000:]   #last 2000 as validation\n",
        "\n",
        "train_paths = paths_subset[train_idxs]\n",
        "val_paths = paths_subset[val_idxs]\n",
        "\n",
        "print(len(train_paths), len(val_paths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltf-r1UA_vAW"
      },
      "source": [
        "\n",
        "## Dataset and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sddxQrhl_pgp"
      },
      "outputs": [],
      "source": [
        "SIZE = 256\n",
        "class ColorizationDataset(Dataset):\n",
        "    def __init__(self, paths, split='train'):\n",
        "        if split == 'train':\n",
        "            self.transforms = transforms.Compose([\n",
        "                transforms.Resize((SIZE, SIZE),  Image.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(), # A little data augmentation!\n",
        "            ])\n",
        "        elif split == 'val':\n",
        "            self.transforms = transforms.Resize((SIZE, SIZE),  Image.BICUBIC)\n",
        "\n",
        "        self.split = split\n",
        "        self.size = SIZE\n",
        "        self.paths = paths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        img = self.transforms(img)\n",
        "        img = np.array(img)\n",
        "        img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b\n",
        "        img_lab = transforms.ToTensor()(img_lab)\n",
        "        L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n",
        "        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n",
        "\n",
        "        return {'L': L, 'ab': ab}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "def make_dataloaders(batch_size=32, n_workers=4, pin_memory=True, **kwargs): # A handy function to make our dataloaders\n",
        "    dataset = ColorizationDataset(**kwargs)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n",
        "                            pin_memory=pin_memory)\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJaAxaga_0gl",
        "outputId": "ec1a8923-9e92-4e22-aee5-8e0df00e8af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n",
            "250 63\n"
          ]
        }
      ],
      "source": [
        "train_dl = make_dataloaders(paths=train_paths, split='train')\n",
        "val_dl = make_dataloaders(paths=val_paths, split='val')\n",
        "\n",
        "data = next(iter(train_dl))\n",
        "Ls, abs_ = data['L'], data['ab']\n",
        "print(Ls.shape, abs_.shape)\n",
        "print(len(train_dl), len(val_dl))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxXlAupwCGDT"
      },
      "source": [
        "\n",
        "## Patch Discriminator\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu-9C9aDCIEj"
      },
      "outputs": [],
      "source": [
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, input_c, num_filters=64, n_down=3):\n",
        "        super().__init__()\n",
        "        model = [self.get_layers(input_c, num_filters, norm=False)]\n",
        "        for i in range(n_down):\n",
        "            model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=2)] # stride of 2 for the block in this loop\n",
        "\n",
        "        model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)] # Make sure to not use normalization or\n",
        "                                                                                             # activation for the last layer of the model\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True): # when needing to make some repeatitive blocks of layers,\n",
        "        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]          # it's always helpful to make a separate method for that purpose\n",
        "        if norm: layers += [nn.BatchNorm2d(nf)]\n",
        "        if act: layers += [nn.LeakyReLU(0.2, True)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3o53EwVCNzI",
        "outputId": "3fcefc7c-cbee-43d4-9140-615621497e9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 1, 15, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model = PatchDiscriminator(3)\n",
        "dummy_input = torch.randn(16, 3, 256, 256) # batch_size, channels, size, size\n",
        "out = model(dummy_input)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L47rCsG4CRnt",
        "outputId": "10911cbc-1cb7-40e5-9d87-4057426f14cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
            "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
            "            Conv2d-3          [-1, 128, 64, 64]         131,072\n",
            "       BatchNorm2d-4          [-1, 128, 64, 64]             256\n",
            "         LeakyReLU-5          [-1, 128, 64, 64]               0\n",
            "            Conv2d-6          [-1, 256, 32, 32]         524,288\n",
            "       BatchNorm2d-7          [-1, 256, 32, 32]             512\n",
            "         LeakyReLU-8          [-1, 256, 32, 32]               0\n",
            "            Conv2d-9          [-1, 512, 16, 16]       2,097,152\n",
            "      BatchNorm2d-10          [-1, 512, 16, 16]           1,024\n",
            "        LeakyReLU-11          [-1, 512, 16, 16]               0\n",
            "           Conv2d-12            [-1, 1, 15, 15]           8,193\n",
            "================================================================\n",
            "Total params: 2,765,633\n",
            "Trainable params: 2,765,633\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 37.00\n",
            "Params size (MB): 10.55\n",
            "Estimated Total Size (MB): 48.30\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "discriminator = PatchDiscriminator(input_c=3, num_filters=64, n_down=3)\n",
        "input_shape = (3, 256, 256)  # (channels, height, width)\n",
        "\n",
        "#Print the summary of the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torchsummary.summary(model.to(device), input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6fMrj6XLh2C"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VPwbsvjCTCk"
      },
      "outputs": [],
      "source": [
        "class UnetBlock(nn.Module):\n",
        "    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n",
        "                 innermost=False, outermost=False):\n",
        "        super().__init__()\n",
        "        self.outermost = outermost\n",
        "        if input_c is None: input_c = nf\n",
        "        downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n",
        "                             stride=2, padding=1, bias=False)\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "        downnorm = nn.BatchNorm2d(ni)\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = nn.BatchNorm2d(nf)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
        "                                        stride=2, padding=1)\n",
        "            down = [downconv]\n",
        "            up = [uprelu, upconv, nn.Tanh()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n",
        "                                        stride=2, padding=1, bias=False)\n",
        "            down = [downrelu, downconv]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
        "                                        stride=2, padding=1, bias=False)\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            if dropout: up += [nn.Dropout(0.5)]\n",
        "            model = down + [submodule] + up\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:\n",
        "            return torch.cat([x, self.model(x)], 1)\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n",
        "        super().__init__()\n",
        "        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n",
        "        for _ in range(n_down - 5):\n",
        "            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n",
        "        out_filters = num_filters * 8\n",
        "        for _ in range(3):\n",
        "            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n",
        "            out_filters //= 2\n",
        "        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg6uAgWSUUJ0",
        "outputId": "9303f5cc-cd67-4b8f-88fb-1cc71cca144d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           1,024\n",
            "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
            "            Conv2d-3          [-1, 128, 64, 64]         131,072\n",
            "       BatchNorm2d-4          [-1, 128, 64, 64]             256\n",
            "         LeakyReLU-5          [-1, 128, 64, 64]               0\n",
            "            Conv2d-6          [-1, 256, 32, 32]         524,288\n",
            "       BatchNorm2d-7          [-1, 256, 32, 32]             512\n",
            "         LeakyReLU-8          [-1, 256, 32, 32]               0\n",
            "            Conv2d-9          [-1, 512, 16, 16]       2,097,152\n",
            "      BatchNorm2d-10          [-1, 512, 16, 16]           1,024\n",
            "        LeakyReLU-11          [-1, 512, 16, 16]               0\n",
            "           Conv2d-12            [-1, 512, 8, 8]       4,194,304\n",
            "      BatchNorm2d-13            [-1, 512, 8, 8]           1,024\n",
            "        LeakyReLU-14            [-1, 512, 8, 8]               0\n",
            "           Conv2d-15            [-1, 512, 4, 4]       4,194,304\n",
            "      BatchNorm2d-16            [-1, 512, 4, 4]           1,024\n",
            "        LeakyReLU-17            [-1, 512, 4, 4]               0\n",
            "           Conv2d-18            [-1, 512, 2, 2]       4,194,304\n",
            "      BatchNorm2d-19            [-1, 512, 2, 2]           1,024\n",
            "        LeakyReLU-20            [-1, 512, 2, 2]               0\n",
            "           Conv2d-21            [-1, 512, 1, 1]       4,194,304\n",
            "             ReLU-22            [-1, 512, 1, 1]               0\n",
            "  ConvTranspose2d-23            [-1, 512, 2, 2]       4,194,304\n",
            "      BatchNorm2d-24            [-1, 512, 2, 2]           1,024\n",
            "        UnetBlock-25           [-1, 1024, 2, 2]               0\n",
            "             ReLU-26           [-1, 1024, 2, 2]               0\n",
            "  ConvTranspose2d-27            [-1, 512, 4, 4]       8,388,608\n",
            "      BatchNorm2d-28            [-1, 512, 4, 4]           1,024\n",
            "          Dropout-29            [-1, 512, 4, 4]               0\n",
            "        UnetBlock-30           [-1, 1024, 4, 4]               0\n",
            "             ReLU-31           [-1, 1024, 4, 4]               0\n",
            "  ConvTranspose2d-32            [-1, 512, 8, 8]       8,388,608\n",
            "      BatchNorm2d-33            [-1, 512, 8, 8]           1,024\n",
            "          Dropout-34            [-1, 512, 8, 8]               0\n",
            "        UnetBlock-35           [-1, 1024, 8, 8]               0\n",
            "             ReLU-36           [-1, 1024, 8, 8]               0\n",
            "  ConvTranspose2d-37          [-1, 512, 16, 16]       8,388,608\n",
            "      BatchNorm2d-38          [-1, 512, 16, 16]           1,024\n",
            "          Dropout-39          [-1, 512, 16, 16]               0\n",
            "        UnetBlock-40         [-1, 1024, 16, 16]               0\n",
            "             ReLU-41         [-1, 1024, 16, 16]               0\n",
            "  ConvTranspose2d-42          [-1, 256, 32, 32]       4,194,304\n",
            "      BatchNorm2d-43          [-1, 256, 32, 32]             512\n",
            "        UnetBlock-44          [-1, 512, 32, 32]               0\n",
            "             ReLU-45          [-1, 512, 32, 32]               0\n",
            "  ConvTranspose2d-46          [-1, 128, 64, 64]       1,048,576\n",
            "      BatchNorm2d-47          [-1, 128, 64, 64]             256\n",
            "        UnetBlock-48          [-1, 256, 64, 64]               0\n",
            "             ReLU-49          [-1, 256, 64, 64]               0\n",
            "  ConvTranspose2d-50         [-1, 64, 128, 128]         262,144\n",
            "      BatchNorm2d-51         [-1, 64, 128, 128]             128\n",
            "        UnetBlock-52        [-1, 128, 128, 128]               0\n",
            "             ReLU-53        [-1, 128, 128, 128]               0\n",
            "  ConvTranspose2d-54          [-1, 2, 256, 256]           4,098\n",
            "             Tanh-55          [-1, 2, 256, 256]               0\n",
            "        UnetBlock-56          [-1, 2, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 54,409,858\n",
            "Trainable params: 54,409,858\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.25\n",
            "Forward/backward pass size (MB): 134.27\n",
            "Params size (MB): 207.56\n",
            "Estimated Total Size (MB): 342.08\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "# an instance of the Unet model\n",
        "generator = Unet()\n",
        "\n",
        "input_size = (1, SIZE, SIZE)  # size = 256 defined abve\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "generator = generator.to(device)\n",
        "\n",
        "# the summary of the model\n",
        "summary(generator, input_size=input_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obj1McrHL0F7"
      },
      "source": [
        "# GAN Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKI8Fi2eLxfB"
      },
      "outputs": [],
      "source": [
        "class GANLoss(nn.Module):\n",
        "    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
        "        if gan_mode == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif gan_mode == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "\n",
        "    def get_labels(self, preds, target_is_real):\n",
        "        if target_is_real:\n",
        "            labels = self.real_label\n",
        "        else:\n",
        "            labels = self.fake_label\n",
        "        return labels.expand_as(preds)\n",
        "\n",
        "    def __call__(self, preds, target_is_real):\n",
        "        labels = self.get_labels(preds, target_is_real)\n",
        "        loss = self.loss(preds, labels)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-W5xClyL5dk"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOz9eF4CL30p"
      },
      "outputs": [],
      "source": [
        "def init_weights(net, init='norm', gain=0.02):\n",
        "\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
        "            if init == 'norm':\n",
        "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
        "            elif init == 'xavier':\n",
        "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init == 'kaiming':\n",
        "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "        elif 'BatchNorm2d' in classname:\n",
        "            nn.init.normal_(m.weight.data, 1., gain)\n",
        "            nn.init.constant_(m.bias.data, 0.)\n",
        "\n",
        "    net.apply(init_func)\n",
        "    print(f\"model initialized with {init} initialization\")\n",
        "    return net\n",
        "\n",
        "def init_model(model, device):\n",
        "    model = model.to(device)\n",
        "    model = init_weights(model)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjdLS6luL_Ph"
      },
      "source": [
        "# Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8wavFuEMB1_"
      },
      "outputs": [],
      "source": [
        "class MainModel(nn.Module):\n",
        "    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4,\n",
        "                 beta1=0.9, beta2=0.999, lambda_L1=100.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.lambda_L1 = lambda_L1\n",
        "\n",
        "        if net_G is None:\n",
        "            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n",
        "        else:\n",
        "            self.net_G = net_G.to(self.device)\n",
        "        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
        "        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n",
        "        self.L1criterion = nn.L1Loss()\n",
        "        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
        "        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
        "\n",
        "\n",
        "    # gridsearch ko lagi fit but crashed due to insufficient ram so commented\n",
        "    # def fit(self, train_loader, epochs):\n",
        "    #         \"\"\"\n",
        "    #         Trains the model for a specified number of epochs.\n",
        "\n",
        "    #         Args:\n",
        "    #           train_loader: The training data loader.\n",
        "    #           epochs: The number of training epochs.\n",
        "    #         \"\"\"\n",
        "    #         for epoch in range(epochs):\n",
        "    #             # Training loop\n",
        "    #             for data in train_loader:\n",
        "    #               self.setup_input(data)\n",
        "    #               self.optimize()\n",
        "\n",
        "\n",
        "    def set_requires_grad(self, model, requires_grad=True):\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = requires_grad\n",
        "\n",
        "    def setup_input(self, data):\n",
        "        self.L = data['L'].to(self.device)\n",
        "        self.ab = data['ab'].to(self.device)\n",
        "\n",
        "    def forward(self):\n",
        "        self.fake_color = self.net_G(self.L)\n",
        "\n",
        "    def backward_D(self):\n",
        "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
        "        fake_preds = self.net_D(fake_image.detach())\n",
        "        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n",
        "        real_image = torch.cat([self.L, self.ab], dim=1)\n",
        "        real_preds = self.net_D(real_image)\n",
        "        self.loss_D_real = self.GANcriterion(real_preds, True)\n",
        "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
        "        self.loss_D.backward()\n",
        "\n",
        "    def backward_G(self):\n",
        "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
        "        fake_preds = self.net_D(fake_image)\n",
        "        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n",
        "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n",
        "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
        "        self.loss_G.backward()\n",
        "\n",
        "    def optimize(self):\n",
        "        self.forward()\n",
        "        self.net_D.train()\n",
        "        self.set_requires_grad(self.net_D, True)\n",
        "        self.opt_D.zero_grad()\n",
        "        self.backward_D()\n",
        "        self.opt_D.step()\n",
        "\n",
        "        self.net_G.train()\n",
        "        self.set_requires_grad(self.net_D, False)\n",
        "        self.opt_G.zero_grad()\n",
        "        self.backward_G()\n",
        "        self.opt_G.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yUEvt9zUKM1"
      },
      "source": [
        "## PSNR for GridSearch\n",
        "\"crash khayo so commented\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BaVw5TNOvaJ"
      },
      "outputs": [],
      "source": [
        "# from skimage.metrics import peak_signal_noise_ratio\n",
        "\n",
        "# def psnr_scorer(y_true, y_pred):\n",
        "#     psnrs = []\n",
        "#     for i in range(y_true.shape[0]):\n",
        "#         psnrs.append(peak_signal_noise_ratio(y_true[i], y_pred[i]))\n",
        "#     return np.mean(psnrs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHRhHUJIMvAK"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.metrics import make_scorer\n",
        "# from skimage.metrics import peak_signal_noise_ratio\n",
        "\n",
        "# # Define ranges for hyperparameters\n",
        "# lr_G_range = np.logspace(-5, -3, 5)\n",
        "# lr_D_range = np.logspace(-4, -2, 5)\n",
        "# beta1_range = np.linspace(0.4, 0.6, 3)\n",
        "# beta2_range = np.linspace(0.995, 0.9995, 3)\n",
        "# lambda_L1_range = np.logspace(1, 3, 5)\n",
        "\n",
        "# # Define parameter grid\n",
        "# param_grid = {\n",
        "#     \"lr_G\": lr_G_range,\n",
        "#     \"lr_D\": lr_D_range,\n",
        "#     \"beta1\": beta1_range,\n",
        "#     \"beta2\": beta2_range,\n",
        "#     \"lambda_L1\": lambda_L1_range,\n",
        "# }\n",
        "\n",
        "# # Define PSNR scoring function\n",
        "# psnr_scorer = make_scorer(peak_signal_noise_ratio, greater_is_better=True)\n",
        "\n",
        "# # Create the grid search object\n",
        "# grid_search = GridSearchCV(estimator=MainModel(), param_grid=param_grid, cv=5, scoring=psnr_scorer)\n",
        "\n",
        "# # Extract the actual data from train_dl\n",
        "# train_data = []\n",
        "# for batch in train_dl:\n",
        "#     train_data.append(batch)\n",
        "\n",
        "# # Train the model and find the best hyperparameters\n",
        "# grid_search.fit(Ls, abs_)  # Replace Ls with the actual variable used in the data loader for the L channel\n",
        "\n",
        "# # Print the best hyperparameters\n",
        "# print(f\"Best hyperparameter combination: {grid_search.best_params_}\")\n",
        "\n",
        "# # Evaluate the performance of the best model on the validation set\n",
        "# best_model_score = grid_search.best_score_\n",
        "# print(f\"Best model validation score (PSNR): {best_model_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpiKZLqKObfj"
      },
      "outputs": [],
      "source": [
        "# import sklearn\n",
        "# sklearn.metrics.get_scorer_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHsoOr_yMLzN"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x668nvE2MGY_"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.count, self.avg, self.sum = [0.] * 3\n",
        "\n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += count * val\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def create_loss_meters():\n",
        "        loss_D_fake = AverageMeter()\n",
        "        loss_D_real = AverageMeter()\n",
        "        loss_D = AverageMeter()\n",
        "        loss_G_GAN = AverageMeter()\n",
        "        loss_G_L1 = AverageMeter()\n",
        "        loss_G = AverageMeter()\n",
        "\n",
        "        return {'loss_D_fake': loss_D_fake,\n",
        "                'loss_D_real': loss_D_real,\n",
        "                'loss_D': loss_D,\n",
        "                'loss_G_GAN': loss_G_GAN,\n",
        "                'loss_G_L1': loss_G_L1,\n",
        "                'loss_G': loss_G}\n",
        "\n",
        "    def update_losses(model, loss_meter_dict, count):\n",
        "        for loss_name, loss_meter in loss_meter_dict.items():\n",
        "            loss = getattr(model, loss_name)\n",
        "            loss_meter.update(loss.item(), count=count)\n",
        "\n",
        "    def lab_to_rgb(L, ab):\n",
        "        \"\"\"\n",
        "        Takes a batch of images\n",
        "        \"\"\"\n",
        "\n",
        "        L = (L + 1.) * 50.\n",
        "        ab = ab * 110.\n",
        "        Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
        "        rgb_imgs = []\n",
        "        for img in Lab:\n",
        "            img_rgb = lab2rgb(img)\n",
        "            rgb_imgs.append(img_rgb)\n",
        "        return np.stack(rgb_imgs, axis=0)\n",
        "\n",
        "    def visualize(model, data, save=True):\n",
        "        model.net_G.eval()\n",
        "        with torch.no_grad():\n",
        "            model.setup_input(data)\n",
        "            model.forward()\n",
        "        model.net_G.train()\n",
        "        fake_color = model.fake_color.detach()\n",
        "        real_color = model.ab\n",
        "        L = model.L\n",
        "        fake_imgs = lab_to_rgb(L, fake_color)\n",
        "        real_imgs = lab_to_rgb(L, real_color)\n",
        "\n",
        "        fig = plt.figure(figsize=(15, 8))\n",
        "        for i in range(5):\n",
        "            ax = plt.subplot(3, 5, i + 1)\n",
        "            ax.imshow(L[i][0].cpu(), cmap='gray')\n",
        "            ax.axis(\"off\")\n",
        "            ax = plt.subplot(3, 5, i + 1 + 5)\n",
        "            ax.imshow(fake_imgs[i])\n",
        "            ax.axis(\"off\")\n",
        "            ax = plt.subplot(3, 5, i + 1 + 10)\n",
        "            ax.imshow(real_imgs[i])\n",
        "            ax.axis(\"off\")\n",
        "        if save:\n",
        "            fig.savefig(f\"colorization_{time.time()}.png\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def log_results(loss_meter_dict):\n",
        "        for loss_name, loss_meter in loss_meter_dict.items():\n",
        "            print(f\"{loss_name}: {loss_meter.avg:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzo5zqzIMSQF"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "52PrbimgcfRi",
        "outputId": "696084c3-5b05-44e9-d9f4-91cd9388feb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [epoch, fake_loss_d, real_loss_d, loss_d, loss_g_GAN, loss_g_L1, loss_g]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c816165-96c1-4f53-9fe0-8e905396689c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>fake_loss_d</th>\n",
              "      <th>real_loss_d</th>\n",
              "      <th>loss_d</th>\n",
              "      <th>loss_g_GAN</th>\n",
              "      <th>loss_g_L1</th>\n",
              "      <th>loss_g</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c816165-96c1-4f53-9fe0-8e905396689c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c816165-96c1-4f53-9fe0-8e905396689c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c816165-96c1-4f53-9fe0-8e905396689c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import pandas as pd ;\n",
        "loss_table = pd.DataFrame(columns = ['epoch', 'fake_loss_d', 'real_loss_d', 'loss_d', 'loss_g_GAN', 'loss_g_L1', 'loss_g']) ;\n",
        "loss_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "QPpyt6k6MNbF",
        "outputId": "e741bbc7-9b35-456f-d320-b6caa3293746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model initialized with norm initialization\n",
            "model initialized with norm initialization\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_loss_meters' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2307799dad34>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-2307799dad34>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dl, epochs, display_every)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mall_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss_meter_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_loss_meters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# function returing a dictionary of objects to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m                                  \u001b[0;31m# log the losses of the complete network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_loss_meters' is not defined"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_dl, epochs, display_every=250):\n",
        "    data = next(iter(val_dl)) # getting a batch for visualizing the model output after fixed intrvals\n",
        "    all_losses = []\n",
        "    for e in range(epochs):\n",
        "        loss_meter_dict = create_loss_meters() # function returing a dictionary of objects to\n",
        "        i = 0                                  # log the losses of the complete network\n",
        "        for data in tqdm(train_dl):\n",
        "            model.setup_input(data)\n",
        "            model.optimize()\n",
        "            update_losses(model, loss_meter_dict, count=data['L'].size(0)) # function updating the log objects\n",
        "            i += 1\n",
        "            if i % display_every == 0:\n",
        "                print(f\"\\nEpoch {e+1}/{epochs}\")\n",
        "                print(f\"Iteration {i}/{len(train_dl)}\")\n",
        "                log_results(loss_meter_dict) # function to print out the losses\n",
        "                visualize(model, data, save=False) # function displaying the model's outputs\n",
        "        all_losses.append(np.array([meter.avg for meter in loss_meter_dict.values()]))\n",
        "        loss_table.loc[len(loss_table.index)] = [e+1] + [meter.avg for meter in loss_meter_dict.values()]\n",
        "        print(all_losses)\n",
        "        torch.save({'model_state_dict': model.state_dict(), 'losses': all_losses}, f'model_epoch_{e+1}.pth')\n",
        "\n",
        "model = MainModel()\n",
        "train_model(model, train_dl, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "227uLJGRhkoh"
      },
      "outputs": [],
      "source": [
        "loss_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRyng5bwEUoA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt ;\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 3)) ;\n",
        "for i in range(3):\n",
        "  axes[i].set_title(loss_table.columns[i+1]) ;\n",
        "  axes[i].plot(loss_table['epoch'], loss_table[loss_table.columns[i+1]]) ;\n",
        "  axes[i].legend(loss_table.columns[i+1]) ;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJlnJ3hzfGKD"
      },
      "outputs": [],
      "source": [
        "from skimage.metrics import structural_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_ssim = np.mean(ssim_scores)\n",
        "print(f\"Average SSIM across batch: {average_ssim:.4f}\")\n"
      ],
      "metadata": {
        "id": "PNZHfEI2AW9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ssim_scores)"
      ],
      "metadata": {
        "id": "NFd2CRBxCDEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JHOJ7mwmC1Eq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}